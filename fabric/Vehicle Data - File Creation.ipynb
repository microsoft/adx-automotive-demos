{"cells":[{"cell_type":"markdown","id":"4c63e3d1-5b50-4ee0-8003-6e48d3c55880","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["# Create a Sample MDF File\n","\n","The code creates a sample MDF-4 file that contains 3 different entries using the asammdf library.\n","\n","The sample MDF file has three signals related to the Engine Control Unit:\n","- RPM\n","- Speed\n","- Gear\n","\n","The file will be stored in the filesystem"]},{"cell_type":"code","execution_count":14,"id":"f6716337-e18b-4f6c-9197-a3c1df4eb63c","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2024-06-10T15:43:21.912923Z","execution_start_time":"2024-06-10T15:43:20.8853722Z","livy_statement_state":"available","parent_msg_id":"b5bc9d36-6015-417f-88c5-5ea414617b66","queued_time":"2024-06-10T15:43:19.4387611Z","session_id":"5b391492-66c4-41d7-ab89-85afb0471038","session_start_time":null,"spark_pool":null,"state":"finished","statement_id":18,"statement_ids":[18]},"text/plain":["StatementMeta(, 5b391492-66c4-41d7-ab89-85afb0471038, 18, Finished, Available)"]},"metadata":{},"output_type":"display_data"}],"source":["import asammdf\n","from asammdf import Source\n","import numpy as np\n","from asammdf.blocks import v4_constants as v4c\n","import numpy as np\n","import uuid\n","\n","\n","# How many samples we will generate\n","numberOfValues = 10000\n","\n","# Create an empty MDF file\n","mdf = asammdf.MDF()\n","\n","# Generate time array\n","time = np.linspace(0, 100, numberOfValues)\n","\n","signals = []\n","\n","source = Source(source_type=Source.SOURCE_TOOL, bus_type=Source.BUS_TYPE_NONE, name=\"EngineControlUnit\", path=\"PT_CAN.Powertrain\", comment=\"Generated\" )\n","\n","# Generate vehicle RPM signal\n","rpm_amplitude = 500\n","rpm_frequency = 10\n","rpm_phase = np.pi/4\n","vehicle_RPM = 3000 + rpm_amplitude * np.sin(2 * np.pi * rpm_frequency * time + rpm_phase) + np.random.normal(0, 50, size=numberOfValues)\n","signals.append(asammdf.Signal(name=\"EngineRPM\", samples=vehicle_RPM, timestamps=time, unit=\"RPM\", source=source))\n","\n","# Generate vehicle speed signal\n","speed_amplitude = 10\n","speed_frequency = 10\n","speed_phase = np.pi/6\n","vehicle_speed = 60 + speed_amplitude * np.sin(2 * np.pi * speed_frequency * time + speed_phase) + np.random.normal(0, 2, size=numberOfValues)\n","signals.append(asammdf.Signal(name=\"Speed\", samples=vehicle_speed, timestamps=time, unit=\"km/h\", source=source))\n","\n","# Generate engine power signal\n","engine_power = vehicle_RPM * vehicle_speed / 1000 + np.random.normal(0, 50, size=numberOfValues)\n","signals.append(asammdf.Signal(name=\"EnginePower\", samples=engine_power, timestamps=time, unit=\"kW\", source=source))\n","\n","# Generate gear signal\n","gear = np.zeros(numberOfValues)\n","for i in range(numberOfValues):\n","    if vehicle_speed[i] < 20:\n","        gear[i] = 1\n","    elif vehicle_speed[i] < 40:\n","        gear[i] = 2\n","    elif vehicle_speed[i] < 60:\n","        gear[i] = 3\n","    else:\n","        gear[i] = 4\n","signals.append(asammdf.Signal(name=\"Gear\", samples=gear, timestamps=time, unit=\"-\", source=source))\n","\n","mdf.append(signals, common_timebase=True)\n","\n","mdf.save(dst=\"/lakehouse/default/Files/sample/test.mf4\", overwrite=True)\n","\n","file_uuid = str(uuid.uuid4())\n"]},{"cell_type":"markdown","id":"edd15ac6-284c-44de-805e-efe094040622","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["# Create metadata file\n","This code will create a metadata file based on the MDF file. The metadata file contains information such as the name and signal types."]},{"cell_type":"code","execution_count":null,"id":"e2f799d9-fa54-489a-9c05-3e280678785c","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["import os\n","from asammdf import MDF\n","from datetime import datetime\n","from asammdf.blocks import v4_constants as v4c\n","import json\n","\n","def getSource(mdf, signal):    \n","    '''\n","        Extracts the source information from the MDF-4 file for a given signal\n","    '''\n","\n","    if signal.source is not None:\n","        source_name = signal.source.name\n","        source_type = v4c.SOURCE_TYPE_TO_STRING[signal.source.source_type]\n","        bus_type = v4c.BUS_TYPE_TO_STRING[signal.source.bus_type]\n","    else:\n","        source_name = \"Unknown\"\n","        source_type = \"Unknown\"\n","        bus_type = \"Unknown\"\n","\n","    try: \n","        channel_group_acq_name = mdf.groups[signal.group_index].channel_group.acq_name\n","    except:\n","        channel_group_acq_name = \"\"\n","\n","    try: \n","        acq_source_name = mdf.groups[signal.group_index].channel_group.acq_source.name\n","    except:\n","        acq_source_name = \"\"\n","\n","    try:\n","        acq_source_path = mdf.groups[signal.group_index].channel_group.acq_source.path\n","    except:\n","        acq_source_path = \"\"\n","\n","    try:\n","        channel_group_acq_source_comment = mdf.groups[signal.group_index].channel_group.acq_source.comment\n","    except:\n","        channel_group_acq_source_comment = \"\"\n","\n","    try:\n","        channel_group_comment = mdf.groups[signal.group_index].channel_group.comment\n","    except:\n","        channel_group_comment = \"\"\n","\n","    try:\n","        signal_source_path = signal.source.path\n","    except:\n","        signal_source_path = \"\"\n","\n","    return source_name, source_type, bus_type, channel_group_acq_name, acq_source_name, acq_source_path, channel_group_acq_source_comment, channel_group_comment, signal_source_path\n","\n","def calculateMetadata(filename, uuid):\n","\n","    mdf = MDF(filename)\n","\n","    print(f\"Generating metadata file {filename}-{uuid}\")\n","\n","    metadata = {\n","        \"name\": filename,\n","        \"source_uuid\": str(uuid),\n","        \"preparation_startDate\": str(datetime.utcnow()),\n","        \"signals\": [],\n","        \"signals_comment\": [],\n","        \"signals_decoding\": [],\n","        \"group_comment\": [],\n","        \"comments\": mdf.header.comment,\n","        \"mdf_start_time\": str(mdf.start_time),\n","    }\n","    \n","    for signal in mdf.iter_channels(raw=True):\n","\n","        source_name, source_type, bus_type, channel_group_acq_name, acq_source_name, acq_source_path, channel_group_acq_source_comment, channel_group_comment, signal_source_path = getSource(mdf, signal)\n","\n","        metadata[\"signals\"].append(\n","            {\n","                \"name\": signal.name,\n","                \"unit\": signal.unit,\n","                \"group_index\": signal.group_index,\n","                \"channel_index\": signal.channel_index,\n","                \"channel_group_acq_name\": channel_group_acq_name,\n","                \"acq_source_name\": acq_source_name,\n","                \"acq_source_path\": acq_source_path,\n","                \"source\" : source_name,\n","                \"source_type\": source_type,\n","                \"bus_type\": bus_type,\n","                \"datatype\": signal.samples.dtype.name,\n","                \"signal_source_path\": signal_source_path,\n","            }          \n","        )\n","\n","        metadata[\"signals_comment\"].append(signal.comment)\n","\n","        metadata[\"signals_decoding\"].append(str(signal.conversion))\n","\n","        metadata[\"group_comment\"].append(\n","            {\n","                \"channel_group_acq_source_comment\": channel_group_acq_source_comment,\n","                \"channel_group_comment\": channel_group_comment\n","            }\n","        )\n","   \n","    print(f\"Finished calculating metadata {file_uuid} with {len(metadata['signals'])} signals\")\n","\n","    mdf.close()\n","\n","    del mdf\n","\n","    return metadata\n","\n","target = \"/lakehouse/default/Files/metadata\"\n","with open(os.path.join(target, f\"{file_uuid}.metadata.json\"), 'w') as metadataFile:\n","    metadata = calculateMetadata(\"/lakehouse/default/Files/sample/test.mf4\", file_uuid)\n","    metadataFile.write(json.dumps(metadata))\n","    print(f\"Finished writing metadata file {file_uuid} with {len(metadata['signals'])} signals\")\n"]},{"cell_type":"markdown","id":"38379da4-475e-4363-af59-bf9970d3f0f5","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["# Decode the MDF file to parquet\n","This script will take the generated MDF file and create a parquet file"]},{"cell_type":"code","execution_count":null,"id":"105360ac-ea42-4474-9a59-2c4519829f72","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["from asammdf import MDF\n","import pyarrow as pa\n","import pyarrow.parquet as pq\n","import re\n","import os\n","\n","\n","def extractSignalsByType(decodedSignal, rawSignal):\n","    '''\n","        Extracts the signals from the MDF-4 file and converts them to a numeric or string representation\n","        Takes into consideration numbers, strings and records (rendered as a string) \n","\n","        We have to make sure that we have the right type / storage based on the datatype.\n","        Trying to use the wrong type will create issues related to loss of precision.\n","\n","        ADX real datatype is a 64 bit float.\n","        This means that all integer types except uint64 and int64 can be stored without loss of precision        \n","\n","    '''   \n","    numberOfSamples = len(decodedSignal.timestamps)\n","\n","    # create an empty array for each type of signal initialized to nan or zero values\n","    floatSignals = np.full(numberOfSamples, np.nan, dtype=np.double)\n","    stringSignals = np.empty(numberOfSamples, dtype=str)\n","\n","    try:\n","        # If it is a record we will decompose its contents on the string field\n","        # we will not store a value in floatSignals\n","        if np.issubdtype(decodedSignal.samples.dtype, np.record):\n","            stringSignals = [record.pprint() for record in decodedSignal.samples]\n","\n","        # If the value can be represented as a float is the only thing we need.\n","        # String will be empty\n","        elif np.issubdtype(decodedSignal.samples.dtype, np.floating):\n","            floatSignals = decodedSignal.samples\n","\n","        # Check if decodedSignal.samples.dtype is a uint64 or uint. If it is, we will only store it as string\n","        # Floats will not be stored as there is a loss of precision\n","        elif np.issubdtype(decodedSignal.samples.dtype, np.uint64) or np.issubdtype(decodedSignal.samples.dtype, np.int64):        \n","            stringSignals = decodedSignal.samples.astype(str)   \n","    \n","        # We will store all ints smaller or equal to 32 bits in floats only, as we have no loss of precision\n","        elif np.issubdtype(decodedSignal.samples.dtype, np.integer):\n","            floatSignals = decodedSignal.samples\n","        \n","        # If we have a pure string as raw signal, we will store it as a string\n","        elif np.issubdtype(rawSignal.samples.dtype, np.string_) or np.issubdtype(rawSignal.samples.dtype, np.unicode_):\n","            stringSignals = rawSignal.samples.view(np.chararray).decode('utf-8') \n","\n","        # For everything else use the previous approach but we will use decode with utf-8 to make sure we get the correct representation for text tables\n","        # astype(string) was causing issues with special characters, and S32 would have truncated results.\n","        else:\n","            floatSignals = rawSignal.samples.astype(float)\n","            stringSignals = decodedSignal.samples.view(np.chararray).decode('utf-8') \n","            \n","\n","    except Exception as e:\n","        print(f\"Exception for {decodedSignal.name}: {e}\")        \n","        print(traceback.print_exc())        \n","        raise e\n","\n","    return floatSignals, stringSignals\n","\n","\n","# Open the MDF file and select a single signal\n","mdf = asammdf.MDF(\"/lakehouse/default/Files/sample/test.mf4\")   \n","\n","\n","for signal in mdf.iter_channels(raw=True):\n","    \n","    group_index = signal.group_index\n","    channel_index = signal.channel_index\n","\n","    # We select a specific signal, both decoded and raw\n","    decodedSignal = mdf.select(channels=[(None, group_index, channel_index)])[0]\n","    rawSignal = mdf.select(channels=[(None, group_index, channel_index)], raw=True)[0]\n","\n","    numberOfSamples = len(decodedSignal.timestamps)\n","\n","    floatSignals, stringSignals = extractSignalsByType(decodedSignal=decodedSignal, rawSignal=rawSignal)                       \n","\n","    table = pa.table (\n","        {                   \n","            \"source_uuid\": np.full(numberOfSamples, file_uuid, dtype=object),\n","            \"group_index\": np.full(numberOfSamples, group_index, dtype=np.int32),\n","            \"channel_index\": np.full(numberOfSamples, channel_index, dtype=np.int32),\n","            \"name\": np.full(numberOfSamples, decodedSignal.name, dtype=object),\n","            \"timestamp\": decodedSignal.timestamps,\n","            \"value\": floatSignals,\n","            \"value_string\": stringSignals,\n","            \"valueRaw\" : rawSignal.samples,\n","        }\n","    )\n","\n","    # Escape all characters from the decodedSignal.name and use only alphanumeric and underscore for the basename\n","    # This is to avoid issues with the basename_template and parquet\n","    parquetFileName = re.sub(r\"[^a-zA-Z0-9_]\", \"_\", decodedSignal.name)\n","\n","    #root_path= os.path.join(\"/lakehouse/default/Files/raw\", file_uuid)\n","    root_path= \"/lakehouse/default/Files/raw\"\n","    if not os.path.exists(root_path):\n","        os.makedirs(root_path)\n","\n","    pq.write_to_dataset(\n","        table, \n","        root_path=root_path,\n","        partition_cols=[\"source_uuid\", \"name\"],\n","        basename_template=f\"{file_uuid}-{group_index}-{channel_index}-{parquetFileName}-{{i}}.parquet\",\n","        use_threads=True,\n","        compression=\"snappy\")                   \n","\n","mdf.close()"]},{"cell_type":"markdown","id":"cd7c931f-dce5-4f13-a369-c0a4cd51b86b","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["# Query the Kusto database\n","\n","The following query will query the database. It requires mounting the generated parquet file as a shortcut."]},{"cell_type":"code","execution_count":null,"id":"9b170bc3-a84c-4758-bf20-87c7943087e2","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["kustoUri = \"https://xxxxxxxx.kusto.fabric.microsoft.com\"\n","database = \"VehicleData\"\n","kustoQuery = f\"external_table('raw') | where source_uuid == guid({file_uuid}) | summarize count() by source_uuid\"\n","\n","kustoDf  = spark.read\\\n","            .format(\"com.microsoft.kusto.spark.synapse.datasource\")\\\n","            .option(\"accessToken\", mssparkutils.credentials.getToken(kustoUri))\\\n","            .option(\"kustoCluster\", kustoUri)\\\n","            .option(\"kustoDatabase\", database) \\\n","            .option(\"kustoQuery\", kustoQuery).load()\n","\n","kustoDf.show()\n","\n"]}],"metadata":{"dependencies":{"environment":{"environmentId":"6cd2ed90-56a4-4e4c-b6c7-899474aa3273","workspaceId":"b9967b8b-d3ce-436a-8d1d-f06c6c053d22"},"lakehouse":{"default_lakehouse":"1d2834ea-9387-4fac-b428-164283434af6","default_lakehouse_name":"VehicleData","default_lakehouse_workspace_id":"b9967b8b-d3ce-436a-8d1d-f06c6c053d22"}},"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"widgets":{}},"nbformat":4,"nbformat_minor":5}
